package de.tum.in.mulan.classifier.transformation;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.ByteArrayInputStream;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.io.StringReader;
import java.text.DecimalFormat;
import java.util.Map;

import mulan.classifier.InvalidDataException;
import mulan.classifier.ModelInitializationException;
import mulan.classifier.MultiLabelLearner;
import mulan.classifier.MultiLabelOutput;
import mulan.classifier.transformation.BinaryRelevance;
import mulan.data.LabelNodeImpl;
import mulan.data.LabelsMetaDataImpl;
import mulan.data.MultiLabelInstances;
import weka.classifiers.AbstractClassifier;
import weka.classifiers.Classifier;
import weka.classifiers.lazy.IBk;
import weka.core.Attribute;
import weka.core.DenseInstance;
import weka.core.FastVector;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.converters.ArffSaver;
import weka.core.converters.CSVLoader;
import weka.core.converters.CSVSaver;
import weka.core.converters.ConverterUtils.DataSource;
import weka.core.matrix.Matrix;
import weka.filters.Filter;
import weka.filters.unsupervised.attribute.NumericToBinary;
import de.tum.in.mulan.classifier.meta.AbstractNoble;
import de.tum.in.preprocessing.EnsembleOfClassifierChainsFiller;
import de.tum.in.preprocessing.ValueCoordinate;

/**
 * the noble multi-relational classifier
 * 
 * @author LehnereS
 */

@SuppressWarnings ("deprecation")
public class Noble extends AbstractNoble {
	
	private static boolean sumBooleanVector(final boolean[] bp1, final boolean[] bp2) {
		boolean bipartition = false;
		for (int i = 0; i < bp1.length; i++)
			bipartition = bipartition || bp1[i] && bp2[i];
		return bipartition;
	}
	
	private static double sumDoubleVector(final double[] conf1, final double[] conf2) {
		double pos_confidence = 0;
		for (int i = 0; i < conf1.length; i++)
			pos_confidence += conf1[i] * conf2[i];
		double neg_confidence = 0;
		for (int i = 0; i < conf1.length; i++)
			neg_confidence += (1 - conf1[i]) * (1 - conf2[i]);
		return pos_confidence / (pos_confidence + neg_confidence);
	}
	
	private int				d_genericClasses, b_genericClasses, id, k;
	private final String	dbp_command	= "$DBP/iter-solver/iter-solver -s$IN -D$DOUT -b$BOUT -t$t -k$k";
	private String			dBPdir		= ".";
	private final String	dbpTmpfile_B_prefix	= "dbp_tmp_B", dbpTmpfile_D_prefix = "dbp_tmp_D";
	private String			workingDir;
	private final MultiLabelLearner	leftMatrixClassifier, topMatrixClassifier;
	private double					t, threshold;
	private boolean					keep	= false;
	
	/**
	 * constructing new Noble object
	 * 
	 * @param classifier
	 *            the base classifier to use
	 * @throws Exception
	 *             any exception
	 */
	public Noble(final Classifier classifier) throws Exception {
		leftMatrixClassifier = new BinaryRelevance(AbstractClassifier.makeCopy(classifier));
		topMatrixClassifier = leftMatrixClassifier.makeCopy();
	}
	
	private void addGenericClasses(final Instance left, final Instance top) {
		left.setDataset(null);
		top.setDataset(null);
		for (int i = 0; i < d_genericClasses; i++)
			left.insertAttributeAt(left.numAttributes());
		for (int i = 0; i < b_genericClasses; i++)
			top.insertAttributeAt(top.numAttributes());
	}
	
	@Override
	public void build(final Instances in_left, final Instances in_top, final Instances in_center)
			throws InvalidDataException, Exception {
		
		Instances left, top, center;
		
		if (isTranspose()) {
			center = AbstractNoble.transpose(in_center);
			top = in_left;
			left = in_top;
		} else {
			left = in_left;
			top = in_top;
			center = in_center;
		}
		
		if (isReplaceMissing()) {
			
			AbstractNoble.debug("Filling up missing values...");
			
			// filling missing values in center matrix
			final int numChains = 5;
			
			final EnsembleOfClassifierChainsFiller filler1 = new EnsembleOfClassifierChainsFiller(new IBk(), numChains);
			
			final EnsembleOfClassifierChainsFiller filler2 = new EnsembleOfClassifierChainsFiller(new IBk(), numChains);
			
			final LabelsMetaDataImpl leftMeta = new LabelsMetaDataImpl();
			for (int i = 0; i < center.numAttributes(); i++)
				leftMeta.addRootNode(new LabelNodeImpl(center.attribute(i).name()));
			
			// FIXME hack as there seems to be a bug in WEKA. Cannot merge
			// instances which
			// are generated by deleting from same dataset
			
			final ArffSaver saver = new ArffSaver();
			saver.setInstances(left);
			final String fileleft = "/tmp/left" + id + k + t + System.currentTimeMillis() + ".arff";
			final String fileright = "/tmp/right" + id + k + t + System.currentTimeMillis() + ".arff";
			saver.setFile(new File(fileleft));
			saver.writeBatch();
			saver.setInstances(center);
			saver.setFile(new File(fileright));
			saver.writeBatch();
			
			DataSource source = new DataSource(fileleft);
			final Instances leftcopy = source.getDataSet();
			
			source = new DataSource(fileright);
			final Instances centercopy = source.getDataSet();
			
			final File leftob = new File(fileleft);
			leftob.delete();
			
			final File rightob = new File(fileright);
			rightob.delete();
			
			// FIXME hack end
			
			final Instances allLeft = Instances.mergeInstances(leftcopy, centercopy);
			
			final FastVector<Attribute> atts = new FastVector<>();
			final LabelsMetaDataImpl rightMeta = new LabelsMetaDataImpl();
			for (int i = 0; i < center.numInstances(); i++) {
				final FastVector<String> attVals = new FastVector<>();
				
				attVals.addElement("0");
				attVals.addElement("1");
				
				atts.addElement(new Attribute("att" + i, attVals));
				rightMeta.addRootNode(new LabelNodeImpl("att" + i));
			}
			
			final Instances centertrans = new Instances("transposed", atts, 0);
			
			for (int i = 0; i < center.numAttributes(); i++) {
				final double[] vals = center.attributeToDoubleArray(i);
				centertrans.add(new DenseInstance(1.0, vals));
				
			}
			
			final Instances allRight = Instances.mergeInstances(top, centertrans);
			
			final MultiLabelInstances fromLeft = new MultiLabelInstances(allLeft, leftMeta);
			final MultiLabelInstances fromRight = new MultiLabelInstances(allRight, rightMeta);
			
			AbstractNoble.debug("filling from left...");
			final Map<ValueCoordinate, Double> fillmapleft = filler1.getVotes(fromLeft);
			AbstractNoble.debug("filling from left...done");
			AbstractNoble.debug("filling from right...");
			final Map<ValueCoordinate, Double> fillmaptop = filler2.getVotes(fromRight);
			AbstractNoble.debug("filling from right...done");
			
			for (int i = 0; i < center.numInstances(); i++)
				for (int j = 0; j < center.numAttributes(); j++)
					if (center.instance(i).isMissing(j)) {
						// double val = (summatrix.get(i,j)/20<0.5)?0.0:1.0;
						final double leftdoub = fillmapleft.get(new ValueCoordinate(i, j + left.numAttributes()));
						final double topdoub = fillmaptop.get(new ValueCoordinate(j, i + top.numAttributes()));
						
						final double val = (topdoub + leftdoub) / (numChains * 2) <= 0.5 ? 0.0 : 1.0;
						center.instance(i).setValue(j, val);
					}
			
			AbstractNoble.debug("Done filling up missing values...");
		}
		
		AbstractNoble.debug("converting center to csv...");
		final File tmpCSV = toCSVforDBP(center);
		
		if (!tmpCSV.exists()) throw new Exception("CSV didnt work, no csv tmp file existing");
		AbstractNoble.debug("done");
		
		final File tmpBMatrix =
				new File(workingDir + "/" + dbpTmpfile_B_prefix + "_" + k + "_" + t + "_" + id + ".dbp");
		final File tmpDMatrix =
				new File(workingDir + "/" + dbpTmpfile_D_prefix + "_" + k + "_" + t + "_" + id + ".dbp");
		
		AbstractNoble.debug("executing DBP...");
		
		final Process p =
				Runtime.getRuntime().exec(
						dbp_command.replaceAll("\\$DBP", dBPdir).replaceAll("\\$IN", tmpCSV.getAbsolutePath())
								.replaceAll("\\$BOUT", tmpBMatrix.getAbsolutePath())
								.replaceAll("\\$DOUT", tmpDMatrix.getAbsolutePath())
								.replaceAll("\\$k", Integer.valueOf(k).toString())
								.replaceAll("\\$t", Double.valueOf(t).toString()));
		p.waitFor();
		p.destroy();
		if (!tmpBMatrix.exists() || !tmpDMatrix.exists())
			throw new Exception("DBP didnt work with setting t=" + new DecimalFormat("#.##").format(t) + " k=" + k);
		AbstractNoble.debug("done");
		
		final LabelsMetaDataImpl tmpMeta = new LabelsMetaDataImpl();
		for (int i = 0; i < k; i++)
			tmpMeta.addRootNode(new LabelNodeImpl("class" + i + "_binarized"));
		
		AbstractNoble.debug("reading DBP output...");
		final Instances dbpD = readDBPMatrix(tmpDMatrix, false);
		d_genericClasses = dbpD.numAttributes();
		final Instances dbpB = readDBPMatrix(tmpBMatrix, true);
		b_genericClasses = dbpB.numAttributes();
		
		final MultiLabelInstances finalDataLeft =
				new MultiLabelInstances(Instances.mergeInstances(left, dbpD), tmpMeta);
		final MultiLabelInstances finalDataTop = new MultiLabelInstances(Instances.mergeInstances(top, dbpB), tmpMeta);
		
		if (keep) {
			final ArffSaver saver = new ArffSaver();
			saver.setInstances(finalDataLeft.getDataSet());
			saver.setFile(new File(workingDir + "/DBP/" + "finalDataLeft_" + k + "_" + t + "_" + id + ".arff"));
			saver.writeBatch();
			saver.setInstances(finalDataTop.getDataSet());
			saver.setFile(new File(workingDir + "/DBP/" + "finalDataTop_" + k + "_" + t + "_" + id + ".arff"));
			saver.writeBatch();
		}
		
		tmpDMatrix.delete();
		tmpBMatrix.delete();
		tmpCSV.delete();
		// to make sure
		tmpDMatrix.deleteOnExit();
		tmpBMatrix.deleteOnExit();
		tmpCSV.deleteOnExit();
		
		AbstractNoble.debug("done");
		AbstractNoble.debug("building internal classifier...");
		
		leftMatrixClassifier.build(finalDataLeft);
		topMatrixClassifier.build(finalDataTop);
		
		AbstractNoble.debug("done");
	}
	
	@Override
	public double classifyInstances(final Instance in_left, final Instance in_top) throws InvalidDataException,
			ModelInitializationException, Exception {
		Instance left, top;
		if (isTranspose()) {
			left = in_top;
			top = in_left;
		} else {
			left = in_left;
			top = in_top;
		}
		
		left = new DenseInstance(left);
		top = new DenseInstance(top);
		
		addGenericClasses(left, top);
		
		MultiLabelOutput leftOut = leftMatrixClassifier.makePrediction(left);
		MultiLabelOutput topOut = topMatrixClassifier.makePrediction(top);
		
		leftOut = new MultiLabelOutput(leftOut.getConfidences(), threshold);
		topOut = new MultiLabelOutput(topOut.getConfidences(), threshold);
		
		return Noble.sumBooleanVector(leftOut.getBipartition(), topOut.getBipartition()) ? 1 : 0;
	}
	
	@Override
	public double distributionForInstances(final Instance in_left, final Instance in_top) throws Exception {
		Instance left, top;
		if (isTranspose()) {
			left = in_top;
			top = in_left;
		} else {
			left = in_left;
			top = in_top;
		}
		
		left = new DenseInstance(left);
		top = new DenseInstance(top);
		
		addGenericClasses(left, top);
		
		final double[] leftConfidences = leftMatrixClassifier.makePrediction(left).getConfidences();
		final double[] topConfidences = topMatrixClassifier.makePrediction(top).getConfidences();
		
		return Noble.sumDoubleVector(leftConfidences, topConfidences);
	}
	
	/**
	 * @return the dBPdir
	 */
	public String getdBPdir() {
		return dBPdir;
	}
	
	/**
	 * @return the k
	 */
	public int getK() {
		return k;
	}
	
	/**
	 * @return the t
	 */
	public double getT() {
		return t;
	}
	
	/**
	 * @return threshold
	 */
	public double getThreshold() {
		return threshold;
	}
	
	/**
	 * @return the workingDir
	 */
	public String getWorkingDir() {
		return workingDir;
	}
	
	/**
	 * @return the keep
	 */
	public boolean isKeep() {
		return keep;
	}
	
	@SuppressWarnings ("resource")
	protected Instances readDBPMatrix(final File tmp, final boolean transpose) throws Exception {
		// FIXME still the feeling the headers could be provided by
		// decomposition tool
		// XXX nice show it to me ;)
		final StringBuilder resultMatrix = new StringBuilder();
		final BufferedReader reader = new BufferedReader(new FileReader(tmp));
		
		// header for WEKA
		for (int i = 0; i < k; i++)
			resultMatrix.append(new String("class" + i + " "));
		resultMatrix.append("\n");
		
		final int rows = Integer.valueOf(reader.readLine());
		final int columns = Integer.valueOf(reader.readLine());
		
		// reading the DBP output
		final StringBuilder originalDBPMatrix = new StringBuilder();
		String line;
		while ((line = reader.readLine()) != null)
			originalDBPMatrix.append(line.trim() + "\n");
		reader.close();
		
		// transposing
		if (transpose) {
			final StringReader dbpReader = new StringReader(rows + " " + columns + "\n" + originalDBPMatrix);
			final Matrix transposedMatrix = new Matrix(dbpReader).transpose();
			final String[] transposedMatrixAsString = transposedMatrix.toString().split("\n");
			for (final String matrixLine : transposedMatrixAsString)
				resultMatrix.append(matrixLine.trim() + "\n");
		} else resultMatrix.append(originalDBPMatrix);
		
		// loading instances set from CSV like bytestream
		final CSVLoader loader = new CSVLoader();
		loader.setSource(new ByteArrayInputStream(resultMatrix.toString().getBytes()));
		loader.setFieldSeparator(" ");
		Instances dbpInstances = loader.getDataSet();
		
		// numeric attributes to nominal {0,1}
		final NumericToBinary num2bin = new NumericToBinary();
		// num2bin.setAttributeIndices("first-last");
		num2bin.setIgnoreClass(true);
		num2bin.setInputFormat(dbpInstances);
		dbpInstances = Filter.useFilter(dbpInstances, num2bin);
		
		return dbpInstances;
	}
	
	/**
	 * @param dBPdir
	 *            the dBPdir to set
	 */
	public void setdBPdir(final String dBPdir) {
		this.dBPdir = dBPdir;
	}
	
	/**
	 * @param k
	 *            the k to set
	 */
	public void setK(final int k) {
		this.k = k;
	}
	
	/**
	 * @param keep
	 *            the keep to set
	 */
	public void setKeep(final boolean keep) {
		this.keep = keep;
	}
	
	/**
	 * @param t
	 *            the t to set
	 */
	public void setT(final double t) {
		this.t = t;
	}
	
	/**
	 * sets the threshold for bi-partitioning
	 * 
	 * @param threshold
	 *            a threshold (standard is 0.5)
	 */
	public void setThreshold(final double threshold) {
		this.threshold = threshold;
	}
	
	/**
	 * @param workingDir
	 *            the workingDir to set
	 */
	public void setWorkingDir(final String workingDir) {
		this.workingDir = workingDir;
	}
	
	@SuppressWarnings ("resource")
	protected File toCSVforDBP(final Instances center) throws IOException, FileNotFoundException {
		final CSVSaver csvSaver = new CSVSaver();
		final File tmpFile =
				new File(workingDir + "/tmp_" + System.currentTimeMillis() + k + "_" + t + "_" + id + ".csv");
		csvSaver.setFile(tmpFile);
		csvSaver.setFieldSeparator(" ");
		csvSaver.setInstances(center);
		csvSaver.writeBatch();
		final BufferedReader reader = new BufferedReader(new FileReader(tmpFile));
		reader.readLine();
		final StringBuilder dbpFormat = new StringBuilder();
		dbpFormat.append(center.numInstances() + "\n");
		dbpFormat.append(center.numAttributes() + "\n");
		String line;
		while ((line = reader.readLine()) != null)
			dbpFormat.append(line + "\n");
		reader.close();
		final BufferedWriter writer = new BufferedWriter(new FileWriter(tmpFile));
		writer.write(dbpFormat.toString());
		writer.close();
		return tmpFile;
	}
}
