\relax 
\catcode`"\active
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\bibstyle{styles/bauermaNum}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\select@language{german}
\@writefile{toc}{\select@language{german}}
\@writefile{lof}{\select@language{german}}
\@writefile{lot}{\select@language{german}}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {chapter}{Abstract}{v}{section*.1}}
\citation{Luscombe01}
\citation{human01}
\citation{Bairoch:1996}
\citation{Luscombe01}
\citation{Fayyad96fromdata}
\citation{mitchell97}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction and Related Work}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:IntroductionAndRelatedWork}{{1}{1}{Introduction and Related Work\relax }{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Description of the field}{1}{section.1.1}}
\newlabel{sec:description}{{1.1}{1}{Description of the field\relax }{section.1.1}{}}
\citation{Tsoumakas07}
\citation{Tsoumakas07}
\citation{Dix07}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Example for classification: Columns describe features, the last column represents the class. The class corresponds to the decision if the user plays tennis or not. Examples are showed in rows.  If the tennis court is only available on weekends and the user only plays tennis on sunny days only the first instance will be classified to $yes$, both other instance will be classified to $no$. \relax }}{2}{table.caption.5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:tennis}{{1.1}{2}{Example for classification: Columns describe features, the last column represents the class. The class corresponds to the decision if the user plays tennis or not. Examples are showed in rows.\\ If the tennis court is only available on weekends and the user only plays tennis on sunny days only the first instance will be classified to $yes$, both other instance will be classified to $no$. \relax \relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{2}{section*.6}}
\@writefile{toc}{\contentsline {paragraph}{Applications}{2}{section*.7}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Motivation}{3}{section.1.2}}
\newlabel{theorem}{{1.2}{3}{\relax }{section.1.2}{}}
\citation{Tsoumakas07}
\citation{SVM}
\citation{Quinlan1986}
\citation{mitchell97}
\citation{Tsoumakas07}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Related Work}{4}{section.1.3}}
\newlabel{sec:relatedwork}{{1.3}{4}{Related Work\relax }{section.1.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Notation}{4}{section*.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Multi-Label Classification Methods}{4}{subsection.1.3.1}}
\newlabel{subsec:mlmethods}{{1.3.1}{4}{Multi-Label Classification Methods\relax }{subsection.1.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Transformation based Methods}{4}{section*.9}}
\newlabel{subsubsec:transformation}{{1.3.1}{4}{Transformation based Methods\relax }{section*.9}{}}
\newlabel{para:br}{{1.3.1}{4}{Binary Relevance\relax }{section*.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Binary Relevance}{4}{section*.10}}
\citation{Tsoumakas09}
\citation{Read09}
\citation{Zhang05}
\newlabel{para:lp}{{1.3.1}{5}{Label Powerset\relax }{section*.11}{}}
\@writefile{toc}{\contentsline {paragraph}{Label Powerset}{5}{section*.11}}
\newlabel{para:ms}{{1.3.1}{5}{Multi-Label Stacking\relax }{section*.12}{}}
\@writefile{toc}{\contentsline {paragraph}{Multi-Label Stacking}{5}{section*.12}}
\newlabel{para:cc}{{1.3.1}{5}{Classifier Chains\relax }{section*.13}{}}
\@writefile{toc}{\contentsline {paragraph}{Classifier Chains}{5}{section*.13}}
\@writefile{toc}{\contentsline {subsubsection}{Algorithm-adoption methods}{5}{section*.14}}
\newlabel{subsubsec:adaption}{{1.3.1}{5}{Algorithm-adoption methods\relax }{section*.14}{}}
\newlabel{para:mlknn}{{1.3.1}{5}{ML-kNN\relax }{section*.15}{}}
\@writefile{toc}{\contentsline {paragraph}{ML-kNN}{5}{section*.15}}
\citation{Tsoumakas08}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1.1}{\ignorespaces ML-kNN\relax }}{6}{algorithm.1.1}}
\newlabel{algo:mlknn}{{1.1}{6}{ML-kNN\relax \relax }{algorithm.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{HOMER}{6}{section*.16}}
\newlabel{subsubsec:homer}{{1.3.1}{6}{HOMER\relax }{section*.16}{}}
\citation{Jain99}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Clustering and Feature Selection}{9}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:clusteringandfeatureselection}{{2}{9}{Clustering and Feature Selection\relax }{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Cluster analysis}{9}{section.2.1}}
\newlabel{sec:clustering}{{2.1}{9}{Cluster analysis\relax }{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Clustering Methods}{10}{subsection.2.1.1}}
\newlabel{subsec:clustering}{{2.1.1}{10}{Clustering Methods\relax }{subsection.2.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Hierarchical Clustering}{10}{section*.17}}
\newlabel{subsubsec:hirach}{{2.1.1}{10}{Hierarchical Clustering\relax }{section*.17}{}}
\@writefile{toc}{\contentsline {paragraph}{Linkage criteria}{10}{section*.18}}
\citation{Myers03}
\citation{Myers03}
\citation{Kaufman90}
\citation{MacQueen67}
\newlabel{para:measures}{{2.1.1}{11}{Measures\relax }{section*.19}{}}
\@writefile{toc}{\contentsline {paragraph}{Measures}{11}{section*.19}}
\newlabel{eqn:euclidean}{{2.5}{11}{Measures\relax }{equation.2.1.5}{}}
\newlabel{eqn:manhatten}{{2.6}{11}{Measures\relax }{equation.2.1.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{k-means Clustering}{11}{section*.20}}
\newlabel{subsubsec:kmeans}{{2.1.1}{11}{k-means Clustering\relax }{section*.20}{}}
\citation{Demp77}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.1}{\ignorespaces k-means clustering\relax }}{12}{algorithm.2.1}}
\newlabel{algo:kmeans}{{2.1}{12}{k-means clustering\relax \relax }{algorithm.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Expectation Maximization}{12}{section*.21}}
\newlabel{subsubsec:em}{{2.1.1}{12}{Expectation Maximization\relax }{section*.21}{}}
\citation{Zheng10}
\citation{Cover91}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Feature Selection}{13}{section.2.2}}
\newlabel{sec:featureselection}{{2.2}{13}{Feature Selection\relax }{section.2.2}{}}
\citation{Kira92}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Feature Selection Methods}{14}{subsection.2.2.1}}
\newlabel{sub:featureselectionmethods}{{2.2.1}{14}{Feature Selection Methods\relax }{subsection.2.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Information Gain}{14}{section*.22}}
\newlabel{subsubsec:infogain}{{2.2.1}{14}{Information Gain\relax }{section*.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{Relief}{14}{section*.23}}
\newlabel{subsubsec:relief}{{2.2.1}{14}{Relief\relax }{section*.23}{}}
\newlabel{eq:relief}{{2.14}{14}{Relief\relax }{equation.2.2.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{Correlation based Feature Selection}{14}{section*.24}}
\newlabel{subsubsec:CFS}{{2.2.1}{14}{Correlation based Feature Selection\relax }{section*.24}{}}
\citation{Press88}
\citation{Witten2005}
\citation{Ahmad2004}
\@writefile{toc}{\contentsline {subsubsection}{Other Feature Selection Methods}{15}{section*.25}}
\newlabel{subsubsec:other}{{2.2.1}{15}{Other Feature Selection Methods\relax }{section*.25}{}}
\newlabel{para:symuncert}{{2.2.1}{15}{Symmetric Uncertainty\relax }{section*.26}{}}
\@writefile{toc}{\contentsline {paragraph}{Symmetric Uncertainty}{15}{section*.26}}
\newlabel{eqn:uncert}{{2.18}{15}{Symmetric Uncertainty\relax }{equation.2.2.18}{}}
\newlabel{eqn:symu}{{2.19}{15}{Symmetric Uncertainty\relax }{equation.2.2.19}{}}
\newlabel{para:sigattreval}{{2.2.1}{15}{Significance Attribute Evaluation\relax }{section*.27}{}}
\@writefile{toc}{\contentsline {paragraph}{Significance Attribute Evaluation}{15}{section*.27}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Feature and Label Selection for Multi-Label Classification}{17}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:main}{{3}{17}{Feature and Label Selection for Multi-Label Classification\relax }{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Cluster based splitting}{17}{section.3.1}}
\newlabel{sec:CML}{{3.1}{17}{Cluster based splitting\relax }{section.3.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.1}{\ignorespaces CML: training on dataset $D$\relax }}{18}{algorithm.3.1}}
\newlabel{algo:CMLtraining}{{3.1}{18}{CML: training on dataset $D$\relax \relax }{algorithm.3.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.2}{\ignorespaces CML: predicting on dataset $D$\relax }}{18}{algorithm.3.2}}
\newlabel{algo:CMLpredicting}{{3.2}{18}{CML: predicting on dataset $D$\relax \relax }{algorithm.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Feature- and Cluster based Splitting}{18}{section.3.2}}
\newlabel{sec:FCML}{{3.2}{18}{Feature- and Cluster based Splitting\relax }{section.3.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.3}{\ignorespaces FCML: FeatureSelection\relax }}{19}{algorithm.3.3}}
\newlabel{algo:fcml_fcs}{{3.3}{19}{FCML: FeatureSelection\relax \relax }{algorithm.3.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.4}{\ignorespaces FCML: Clustering\relax }}{19}{algorithm.3.4}}
\newlabel{algo:fcml_clustering}{{3.4}{19}{FCML: Clustering\relax \relax }{algorithm.3.4}{}}
\citation{mulan}
\citation{weka}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments}{21}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:Experiments}{{4}{21}{Experiments\relax }{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Implementation}{21}{section.4.1}}
\newlabel{sec:implemenation}{{4.1}{21}{Implementation\relax }{section.4.1}{}}
\citation{Tsoumakas07}
\citation{DBLP:conf/nips/ElisseeffW01}
\citation{Pestian07w.:a}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Multi-Label Datasets}{22}{section.4.2}}
\newlabel{sec:datasets}{{4.2}{22}{Multi-Label Datasets\relax }{section.4.2}{}}
\newlabel{LCard}{{4.1}{22}{Multi-Label Datasets\relax }{equation.4.2.1}{}}
\newlabel{LDens}{{4.2}{22}{Multi-Label Datasets\relax }{equation.4.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Class diagram showing the implementation structure of this work: self implemented classes are grey, white boxes represent \unhbox \voidb@x \hbox {MULAN} classes. Most important input and output methods are shown. Drawn through lines indicate an "extends"-relationship, dashed lines an "used-by"-relationship.\newline  Main class is the \textit  {GroupBasedMetaClassifier} as it holds the subsets of the data "groups" and the multi-label classifier as well the single-label classifier. For every group a \textit  {FilteredMLLearner} is used to extract the group subset from the dataset. For identifying groups either \textit  {CML} or \textit  {FCML} is used.\relax }}{23}{figure.caption.28}}
\newlabel{img:classDiag}{{4.1}{23}{Class diagram showing the implementation structure of this work: self implemented classes are grey, white boxes represent \mbox {MULAN} classes. Most important input and output methods are shown. Drawn through lines indicate an "extends"-relationship, dashed lines an "used-by"-relationship.\newline Main class is the \textit {GroupBasedMetaClassifier} as it holds the subsets of the data "groups" and the multi-label classifier as well the single-label classifier. For every group a \textit {FilteredMLLearner} is used to extract the group subset from the dataset. For identifying groups either \textit {CML} or \textit {FCML} is used.\relax \relax }{figure.caption.28}{}}
\citation{read:2008}
\citation{Tsoumakas08}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Multi-label datasets used in this work. $N$ is the number of instances, $L$ the number of Labels. $M$ is the number of Features. $n$ indicates numeric features, $b$ binary features\relax }}{24}{table.caption.29}}
\newlabel{tab:datasets}{{4.1}{24}{Multi-label datasets used in this work. $N$ is the number of instances, $L$ the number of Labels. $M$ is the number of Features. $n$ indicates numeric features, $b$ binary features\relax \relax }{table.caption.29}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Evaluation Measures}{24}{section.4.3}}
\newlabel{sec:evalmeasures}{{4.3}{24}{Evaluation Measures\relax }{section.4.3}{}}
\citation{citeulike:8938538}
\citation{Ghamrawi05collectivemultilabel}
\citation{Schapire00boostexter:a}
\citation{Godbole04discriminativemethods}
\newlabel{para:examplemeasures}{{4.3}{25}{Example-based Evaluation Measures\relax }{section*.30}{}}
\@writefile{toc}{\contentsline {paragraph}{Example-based Evaluation Measures}{25}{section*.30}}
\newlabel{eqn:subsetaccuracy}{{4.3}{25}{Example-based Evaluation Measures\relax }{equation.4.3.3}{}}
\newlabel{eqn:hammingloss}{{4.4}{25}{Example-based Evaluation Measures\relax }{equation.4.3.4}{}}
\newlabel{para:otherexample}{{4.3}{25}{\relax }{section*.31}{}}
\newlabel{eqn:precision}{{4.5}{25}{\relax }{equation.4.3.5}{}}
\newlabel{eqn:recall}{{4.6}{25}{\relax }{equation.4.3.6}{}}
\citation{Yang99anevaluation}
\newlabel{eqn:fmeasure}{{4.7}{26}{\relax }{equation.4.3.7}{}}
\newlabel{eqn:accuracy}{{4.8}{26}{\relax }{equation.4.3.8}{}}
\newlabel{para:labelmeasures}{{4.3}{26}{Label-based Evaluation Measures\relax }{section*.32}{}}
\newlabel{eqn:mmacro}{{4.9}{26}{Label-based Evaluation Measures\relax }{equation.4.3.9}{}}
\newlabel{eqn:mmacro}{{4.10}{26}{Label-based Evaluation Measures\relax }{equation.4.3.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Evaluation of Clustering- and Feature Selection Methods}{26}{section.4.4}}
\newlabel{sec:evaluation}{{4.4}{26}{Evaluation of Clustering- and Feature Selection Methods\relax }{section.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Clustering}{27}{subsection.4.4.1}}
\newlabel{subsec:eval_clustering}{{4.4.1}{27}{Clustering\relax }{subsection.4.4.1}{}}
\@writefile{toc}{\contentsline {paragraph}{CML}{27}{section*.33}}
\@writefile{toc}{\contentsline {paragraph}{FCML}{27}{section*.34}}
\newlabel{tab:cal500}{{4.1(a)}{28}{Subtable 4 4.1(a)\relax }{subtable.4.1.1}{}}
\newlabel{sub@tab:cal500}{{(a)}{28}{Subtable 4 4.1(a)\relax }{subtable.4.1.1}{}}
\newlabel{tab:enron}{{4.1(b)}{28}{Subtable 4 4.1(b)\relax }{subtable.4.1.2}{}}
\newlabel{sub@tab:enron}{{(b)}{28}{Subtable 4 4.1(b)\relax }{subtable.4.1.2}{}}
\newlabel{tab:medical}{{4.1(c)}{28}{Subtable 4 4.1(c)\relax }{subtable.4.1.3}{}}
\newlabel{sub@tab:medical}{{(c)}{28}{Subtable 4 4.1(c)\relax }{subtable.4.1.3}{}}
\newlabel{tab:yeast}{{4.1(d)}{28}{Subtable 4 4.1(d)\relax }{subtable.4.1.4}{}}
\newlabel{sub@tab:yeast}{{(d)}{28}{Subtable 4 4.1(d)\relax }{subtable.4.1.4}{}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(a)}{\ignorespaces {ranking in dataset \textit {CAL500}}}}{28}{table.caption.35}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(b)}{\ignorespaces {ranking in dataset \textit {enron}}}}{28}{table.caption.35}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(c)}{\ignorespaces {ranking in dataset \textit {medical}}}}{28}{table.caption.35}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(d)}{\ignorespaces {ranking in dataset \textit {yeast}}}}{28}{table.caption.35}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Ranking of scenarios within different datasets using ClassifierChain as multi-label learner. Only the Top 3 results are shown. Different feature selection methods as well different cluster algorithm were used. The results are ordered along their example-based accuracy.\relax }}{28}{table.caption.35}}
\newlabel{tab:ranking}{{4.2}{28}{Ranking of scenarios within different datasets using ClassifierChain as multi-label learner. Only the Top 3 results are shown. Different feature selection methods as well different cluster algorithm were used. The results are ordered along their example-based accuracy.\relax \relax }{table.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Average Example-based accuracy for different cluster algorithm on CML over different datasets\relax }}{29}{figure.caption.36}}
\newlabel{fig:cml_clus}{{4.2}{29}{Average Example-based accuracy for different cluster algorithm on CML over different datasets\relax \relax }{figure.caption.36}{}}
\newlabel{fig:fcml_clus_acc}{{4.3(a)}{29}{Subfigure 4 4.3(a)\relax }{subfigure.4.3.1}{}}
\newlabel{sub@fig:fcml_clus_acc}{{(a)}{29}{Subfigure 4 4.3(a)\relax }{subfigure.4.3.1}{}}
\newlabel{fig:fcml_clus_fmeasure}{{4.3(b)}{29}{Subfigure 4 4.3(b)\relax }{subfigure.4.3.2}{}}
\newlabel{sub@fig:fcml_clus_fmeasure}{{(b)}{29}{Subfigure 4 4.3(b)\relax }{subfigure.4.3.2}{}}
\newlabel{fig:fcml_clus_hamming}{{4.3(c)}{29}{Subfigure 4 4.3(c)\relax }{subfigure.4.3.3}{}}
\newlabel{sub@fig:fcml_clus_hamming}{{(c)}{29}{Subfigure 4 4.3(c)\relax }{subfigure.4.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Evaluation of clustering algorithms in FCML. Different datasets were used. The measures are averaged over different settings, like $nc$, linkage criteria and distance measure\relax }}{29}{figure.caption.37}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {\relax \fontsize {8}{9.5}\selectfont \par@update {Example-based Accuracy}}}}{29}{figure.caption.37}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {\relax \fontsize {8}{9.5}\selectfont \par@update {Micro-averaged F-Measure}}}}{29}{figure.caption.37}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {\relax \fontsize {8}{9.5}\selectfont \par@update {Hamming Loss}}}}{29}{figure.caption.37}}
\newlabel{fig:fcml_clus}{{4.3}{29}{Evaluation of clustering algorithms in FCML. Different datasets were used. The measures are averaged over different settings, like $nc$, linkage criteria and distance measure\relax \relax }{figure.caption.37}{}}
\newlabel{fig:fcml_clus_meth_acc}{{4.4(a)}{30}{Subfigure 4 4.4(a)\relax }{subfigure.4.4.1}{}}
\newlabel{sub@fig:fcml_clus_meth_acc}{{(a)}{30}{Subfigure 4 4.4(a)\relax }{subfigure.4.4.1}{}}
\newlabel{fig:fcml_clus_meth_fmeasure}{{4.4(b)}{30}{Subfigure 4 4.4(b)\relax }{subfigure.4.4.2}{}}
\newlabel{sub@fig:fcml_clus_meth_fmeasure}{{(b)}{30}{Subfigure 4 4.4(b)\relax }{subfigure.4.4.2}{}}
\newlabel{fig:fcml_clus_meth_hamming}{{4.4(c)}{30}{Subfigure 4 4.4(c)\relax }{subfigure.4.4.3}{}}
\newlabel{sub@fig:fcml_clus_meth_hamming}{{(c)}{30}{Subfigure 4 4.4(c)\relax }{subfigure.4.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Evaluation of hierarchical clustering methods on different datasets. The measures are averaged over different distance measures\relax }}{30}{figure.caption.38}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {\relax \fontsize {8}{9.5}\selectfont \par@update {Example-based Accuracy}}}}{30}{figure.caption.38}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {\relax \fontsize {8}{9.5}\selectfont \par@update {Micro-averaged F-Measure}}}}{30}{figure.caption.38}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {\relax \fontsize {8}{9.5}\selectfont \par@update {Hamming Loss}}}}{30}{figure.caption.38}}
\newlabel{fig:fcml_clus_meth}{{4.4}{30}{Evaluation of hierarchical clustering methods on different datasets. The measures are averaged over different distance measures\relax \relax }{figure.caption.38}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Pearson-correlation matrix for the setting "ClassifierChain, Hierarchical Clustering, single-linkage, Euclidean Distance, InformationGain" \textit  {CAL500} dataset with different $nc$\relax }}{30}{table.caption.42}}
\newlabel{tab:cor}{{4.3}{30}{Pearson-correlation matrix for the setting "ClassifierChain, Hierarchical Clustering, single-linkage, Euclidean Distance, InformationGain" \textit {CAL500} dataset with different $nc$\relax \relax }{table.caption.42}{}}
\newlabel{fig:fcml_clus_dist_acc}{{4.5(a)}{31}{Subfigure 4 4.5(a)\relax }{subfigure.4.5.1}{}}
\newlabel{sub@fig:fcml_clus_dist_acc}{{(a)}{31}{Subfigure 4 4.5(a)\relax }{subfigure.4.5.1}{}}
\newlabel{fig:fcml_clus_dist_fmeasure}{{4.5(b)}{31}{Subfigure 4 4.5(b)\relax }{subfigure.4.5.2}{}}
\newlabel{sub@fig:fcml_clus_dist_fmeasure}{{(b)}{31}{Subfigure 4 4.5(b)\relax }{subfigure.4.5.2}{}}
\newlabel{fig:fcml_clus_dist_hamming}{{4.5(c)}{31}{Subfigure 4 4.5(c)\relax }{subfigure.4.5.3}{}}
\newlabel{sub@fig:fcml_clus_dist_hamming}{{(c)}{31}{Subfigure 4 4.5(c)\relax }{subfigure.4.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Evaluation of distance measure using hierarchical clustering on different datasets. The measures are averaged over different distance measures\relax }}{31}{figure.caption.39}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {\relax \fontsize {8}{9.5}\selectfont \par@update {Example-based Accuracy}}}}{31}{figure.caption.39}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {\relax \fontsize {8}{9.5}\selectfont \par@update {Micro-averaged F-Measure}}}}{31}{figure.caption.39}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {\relax \fontsize {8}{9.5}\selectfont \par@update {Hamming Loss}}}}{31}{figure.caption.39}}
\newlabel{fig:fcml_clus_dist}{{4.5}{31}{Evaluation of distance measure using hierarchical clustering on different datasets. The measures are averaged over different distance measures\relax \relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Example-based Accuracy, Micro-averaged F-Measure, Hamming Loss and average Density with growing number of clusters. "ClassifierChain, Hierarchical Clustering, single-linkage, Euclidean Distance, InformationGain" \textit  {CAL500} dataset with different $nc$.\relax }}{31}{figure.caption.41}}
\newlabel{fig:clu_cor}{{4.6}{31}{Example-based Accuracy, Micro-averaged F-Measure, Hamming Loss and average Density with growing number of clusters. "ClassifierChain, Hierarchical Clustering, single-linkage, Euclidean Distance, InformationGain" \textit {CAL500} dataset with different $nc$.\relax \relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Feature-Selection}{32}{subsection.4.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Evaluation of methods}{32}{subsection.4.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Feature-Selection Methods on the \textit  {CAL500} dataset. Example-based Accuracy is averaged over different clustering settings\relax }}{33}{figure.caption.43}}
\newlabel{fig:fscal}{{4.7}{33}{Feature-Selection Methods on the \textit {CAL500} dataset. Example-based Accuracy is averaged over different clustering settings\relax \relax }{figure.caption.43}{}}
\newlabel{fig:fcml_fs_acc}{{4.8(a)}{33}{Subfigure 4 4.8(a)\relax }{subfigure.4.8.1}{}}
\newlabel{sub@fig:fcml_fs_acc}{{(a)}{33}{Subfigure 4 4.8(a)\relax }{subfigure.4.8.1}{}}
\newlabel{fig:fcml_fs_fmeasure}{{4.8(b)}{33}{Subfigure 4 4.8(b)\relax }{subfigure.4.8.2}{}}
\newlabel{sub@fig:fcml_fs_fmeasure}{{(b)}{33}{Subfigure 4 4.8(b)\relax }{subfigure.4.8.2}{}}
\newlabel{fig:fcml_fs_hamming}{{4.8(c)}{33}{Subfigure 4 4.8(c)\relax }{subfigure.4.8.3}{}}
\newlabel{sub@fig:fcml_fs_hamming}{{(c)}{33}{Subfigure 4 4.8(c)\relax }{subfigure.4.8.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces \textit  {Relief}, \textit  {CFS} and \textit  {Information Gain} feature selection on different datasets\relax }}{33}{figure.caption.44}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {\relax \fontsize {8}{9.5}\selectfont \par@update {Example-based Accuracy}}}}{33}{figure.caption.44}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {\relax \fontsize {8}{9.5}\selectfont \par@update {Micro-averaged F-Measure}}}}{33}{figure.caption.44}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {\relax \fontsize {8}{9.5}\selectfont \par@update {Hamming Loss}}}}{33}{figure.caption.44}}
\newlabel{fig:fs}{{4.8}{33}{\textit {Relief}, \textit {CFS} and \textit {Information Gain} feature selection on different datasets\relax \relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces CML compared to pure multi-label learners using the example-based accuracy. For CML the best result is taken.\relax }}{34}{figure.caption.46}}
\newlabel{fig:cml}{{4.9}{34}{CML compared to pure multi-label learners using the example-based accuracy. For CML the best result is taken.\relax \relax }{figure.caption.46}{}}
\@writefile{toc}{\contentsline {subsubsection}{CML}{34}{section*.45}}
\@writefile{toc}{\contentsline {subsubsection}{FCML}{35}{section*.47}}
\@writefile{toc}{\contentsline {subsubsection}{Summary}{35}{section*.49}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces FCML compared to pure multi-label learners and HOMER on different datasets. In every setting the best result is presented. HOMER did not finish on all scenarios.\relax }}{36}{figure.caption.48}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Example-based Accuracy}}}{36}{figure.caption.48}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Micro-averaged F-Measure}}}{36}{figure.caption.48}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Hamming Loss}}}{36}{figure.caption.48}}
\newlabel{fig:fcml_res}{{4.10}{36}{FCML compared to pure multi-label learners and HOMER on different datasets. In every setting the best result is presented. HOMER did not finish on all scenarios.\relax \relax }{figure.caption.48}{}}
\citation{cheng10icmlmld}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusions}{37}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:Conclusions}{{5}{37}{Conclusions\relax }{chapter.5}{}}
\bibdata{bibliography/literature}
\bibcite{Ahmad2004}{1}
\bibcite{Bairoch:1996}{2}
\bibcite{SVM}{3}
\bibcite{Cover91}{4}
\bibcite{cheng10icmlmld}{5}
\bibcite{Demp77}{6}
\bibcite{Dix07}{7}
\bibcite{DBLP:conf/nips/ElisseeffW01}{8}
\bibcite{Fayyad96fromdata}{9}
\bibcite{Ghamrawi05collectivemultilabel}{10}
\bibcite{Godbole04discriminativemethods}{11}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{39}{chapter*.50}}
\bibcite{weka}{12}
\bibcite{human01}{13}
\bibcite{Jain99}{14}
\bibcite{Kaufman90}{15}
\bibcite{Kira92}{16}
\bibcite{Luscombe01}{17}
\bibcite{MacQueen67}{18}
\bibcite{mitchell97}{19}
\bibcite{Myers03}{20}
\bibcite{Pestian07w.:a}{21}
\bibcite{Press88}{22}
\bibcite{Quinlan1986}{23}
\bibcite{read:2008}{24}
\bibcite{citeulike:8938538}{25}
\bibcite{Read09}{26}
\bibcite{Schapire00boostexter:a}{27}
\bibcite{Tsoumakas09}{28}
\bibcite{Tsoumakas07}{29}
\bibcite{Tsoumakas08}{30}
\bibcite{mulan}{31}
\bibcite{Witten2005}{32}
\bibcite{Yang99anevaluation}{33}
\bibcite{Zhang05}{34}
\bibcite{Zheng10}{35}
