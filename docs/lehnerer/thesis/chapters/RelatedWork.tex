\chapter{Related Work}
\label{chapter:RelatedWork}

This chapter is supposed to give an overview over existing multi-label classification techniques. Especially HOMER is described here, as it uses some similar approach as this work, although with a different intention. The following notation is used for describing the multi-label problems: $ D $ is the dataset composed of n examples $ (x_1,S_1),(x_2,S_2),\hdots,(x_n,S_n) $ where $ x $ is the instance containing the feature values and $ S \subset L $ a subset of labels associated with this instance.\comment{zu aehnlich zu \cite{Read09}?}

\section{Multi-Label Classification Methods}
There are two groups of multi-label classification methods\cite{Tsoumakas07}: 1) transformation-based methods which transform the problem in to several single-label problem and apply well known single label classification methods like support vector machines. 2) algorithm-adoption methods which extend existing algorithms to be capable of handling multi-label data directly.
\subsection{Transformation based Methods}
\subsubsection{Binary Relevance}
A elementary multi-label classification method is Binary Relevance (BR)\comment{leider keinen "erfinder" gefunden, was referenzieren?}. This methods transforms the multi-label problem into $ |L| $ separate problems and trains a single-label learner on each on them. The prediction is simply composed by the prediction of the single classifications $ l_1\hdots l_n $. It ignores any dependences in-between the labels. However is it still popular as it scales linear with the number of labels\comment{auch hier brauchts noch eine referenz}.
\subsubsection{Label Powerset}\comment{referenz...}
The Label Powerset method tries to keep up with label dependencies by taking a higher effort in computational complexity. For every unique set $ S $ of labels in the data a single label $ l_S $ is created representing this combination, and a single-label classifier is build upon it. So every combination is represented as a single label and label correlations come direct into account. Despite the worst-time complexity an other disadvantage is that it can only predict label combinations which have been seen in the training data.
\subsubsection{Multi-Label Stacking}
Multilabel-Stacking or $ BR^2 $ tries to deal with the disadvantage of BR by loosing a label relations\cite{Tsoumakas09}. In order to achieve that a stacking mechanism is used. A meta-level of classifiers (one for each label) is introduced taking as input the output of the $ |L| $ first-level single-label classifications. In this meta-level label-confidences are taken into account, leading to a relations depended prediction. To reduce the noise the $\phi$ correlation between all labels on the original data is computed. For the meta-level only those labels which exceed the threshold in respect to the current label will be used. By this the dimensionality of the feature space in the meta-level and also the noise will be reduced.
\subsubsection{Classifier Chains}
Classifier Chains by Read et al.\cite{Read09} try to handle the label correlations by using a chain of single-label classifiers predicting the labels subsequently. In detail for every label $ l $ in the label set $ L $ a single-label classifier is trained, where every link in the chain has the label associations of all previous links. For predicting the feature space of each link is extended by the prediction of the previous classifier. Although they claim that the ordering of the chain is essential, current implementations are using a random order. Ensemble of Classifier Chains are trying to avoid this ordering problem by using several chains with different orders.
\subsubsection{HOMER}
The multi-label learner introduced by Tsoumakas et al.\cite{Tsoumakas07_2} is based upon a Hierarchy Of Multilabel classifERs. The Hierarchy is build by splitting the label set $ L $ into $k$ disjunct subsets at each level. To ensure a balance instance distribution for each subset a modified cluster algorithm based on k-means is used. For predicting a meta-label is introduced, determining the path through the hierarchy. A classifier which predicts the meta-label, indicates which branch of the tree is used. Therefore only branches with a positive meta-labels will considered for predicting, what reduces the computational costs.
The clustering only considers label information and doesn't group features. However, HOMER has been developed with focus on effectiveness and efficiency and not to use underlying structures in the data.

\subsection{Algorithm-adoption methods}
\subsubsection{Ml-kNN}
Ml-kNN proposed by Min-Ling Zhang and Zhi-Hua Zhou\cite{Zhang07} is an adoption of the popular k-NearestNeighbours algorithm. As a first step the k nearest neighbours are retrieved, secondly the label sets are mapped using the maximum a posteriori principle (MAP) based on the information gain about the label sets of the neighbouring instances.\comment{bissl wenig, oder?}